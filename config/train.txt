model_name_or_path: {base_model}
stage: sft
do_train: true
do_eval: true

finetuning_type: full          
template: llama3               

dataset_dir: dataset

dataset: {train_dataset}_train

eval_dataset: {val_dataset}_val


cutoff_len: 2048
packing: false

output_dir: {output_model}
num_train_epochs: 3
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2e-5

bf16: true
logging_steps: 20
eval_strategy: epoch
save_strategy: epoch
save_total_limit: 2

lr_scheduler_type: cosine
warmup_ratio: 0.03

report_to: none